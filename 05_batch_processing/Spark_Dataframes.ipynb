{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef46443-00eb-4181-b0c3-74dfa17fe0fd",
   "metadata": {},
   "source": [
    "# Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0813938-0d15-40b4-b1c1-757408b0690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da27e0a2-203f-4a52-8c22-ad1a942a379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/25 18:15:42 WARN Utils: Your hostname, DESKTOP-CSJ1S7Q resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/25 18:15:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/25 18:15:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/25 18:15:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\n",
    "    \"local[*]\"\n",
    ").appName(\n",
    "    \"test\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c046fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"fhvhv/2021/01/\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53719e2-7967-4782-a0c7-36ff33d14f00",
   "metadata": {},
   "source": [
    "## Actions and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "103086cb-de38-4a06-8590-ae7ef301c3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-11 19:40:22|2021-01-11 20:15:49|         262|         231|\n",
      "|2021-01-05 16:13:22|2021-01-05 16:27:50|          61|         181|\n",
      "|2021-01-31 19:42:09|2021-01-31 19:59:52|         232|           4|\n",
      "|2021-01-27 23:24:36|2021-01-27 23:26:43|          68|          68|\n",
      "|2021-01-30 09:35:46|2021-01-30 09:39:42|         256|         255|\n",
      "|2021-01-16 03:25:35|2021-01-16 03:34:21|          89|          91|\n",
      "|2021-01-11 12:58:23|2021-01-11 13:14:19|          97|          61|\n",
      "|2021-01-03 08:44:58|2021-01-03 09:04:45|          26|         178|\n",
      "|2021-01-14 19:52:00|2021-01-14 20:19:00|         181|         198|\n",
      "|2021-01-08 21:35:35|2021-01-08 22:06:33|          76|          91|\n",
      "|2021-01-15 14:49:48|2021-01-15 15:35:23|         246|          16|\n",
      "|2021-01-27 11:37:56|2021-01-27 11:53:35|         135|          73|\n",
      "|2021-01-11 18:29:44|2021-01-11 18:42:49|          68|         211|\n",
      "|2021-01-24 22:32:15|2021-01-24 22:52:42|         249|         236|\n",
      "|2021-01-26 19:03:39|2021-01-26 19:10:53|          79|           4|\n",
      "|2021-01-07 09:05:32|2021-01-07 09:30:47|          22|          25|\n",
      "|2021-01-12 18:16:00|2021-01-12 18:25:56|          69|         119|\n",
      "|2021-01-16 22:00:39|2021-01-16 22:18:15|         239|         239|\n",
      "|2021-01-15 00:35:14|2021-01-15 00:45:30|          61|          62|\n",
      "|2021-01-18 00:23:11|2021-01-18 00:34:42|         241|          20|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"pickup_datetime\", \"dropoff_datetime\", \"PULocationID\", \"DOLocationID\") \\\n",
    "    .filter(df.hvfhs_license_num == \"HV0003\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9de3d-e7b7-424c-8547-3bcd4a216086",
   "metadata": {},
   "source": [
    "Transformations: Are not executed immediately but only when needed -> lazy\n",
    "* show, take, head\n",
    "* write\n",
    "\n",
    "Actions: Make the computations happen -> eager\n",
    "* filter\n",
    "* select\n",
    "* join\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0b21c-3077-4b3c-bb1e-9b7d6d8874a4",
   "metadata": {},
   "source": [
    "## User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0be09f6-59b1-4397-889e-ac51d4250485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ceb458a-76da-45a1-87cf-b2a16239faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f\"s/{num}:x\"\n",
    "    return f\"e/{num}:x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51b43513-30e7-429c-a14c-3edc4eb63e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_udf = F.udf(stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b33e364-5801-4e88-af26-0fbc412969e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+--------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID| base_id|\n",
      "+-----------+------------+------------+------------+--------+\n",
      "| 2021-01-11|  2021-01-11|         262|         231|e/2764:x|\n",
      "| 2021-01-05|  2021-01-05|          61|         181|e/2617:x|\n",
      "| 2021-01-02|  2021-01-02|         100|           1|e/2510:x|\n",
      "| 2021-01-31|  2021-01-31|         232|           4|e/2882:x|\n",
      "| 2021-01-05|  2021-01-05|         162|           1|s/2800:x|\n",
      "+-----------+------------+------------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn(\"dropoff_date\", F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn(\"base_id\", stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select(\"pickup_date\", \"dropoff_date\", \"PULocationID\", \"DOLocationID\", \"base_id\") \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
